{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install praw\n",
    "# !pip install requests-cache\n",
    "# !pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "!mkdir -p web_cache\n",
    "requests_cache.install_cache('web_cache/redditcrawler')\n",
    "# requests_cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import JSON\n",
    "from collections import Counter\n",
    "\n",
    "from lib.parallel import parallel\n",
    "from lib.labels_from_tags import labels_from_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mongo = MongoClient('172.17.0.1', 27017)\n",
    "db = mongo['bad-vis']\n",
    "reddit_raw = db['reddit-raw']\n",
    "pushshift_raw = db['pushshift-raw']\n",
    "reddit_merge = db['reddit-merge']\n",
    "reddit_cache = db['reddit-cache']\n",
    "posts = db['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "credentials = \"\"\n",
    "\n",
    "with open('./reddit_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials['client_id'],\n",
    "                     client_secret=credentials['client_secret'],\n",
    "                     password=credentials['password'],\n",
    "                     user_agent=credentials['user_agent'],\n",
    "                     username=credentials['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# break the rate limiter since we have requests cache\n",
    "# from types import MethodType\n",
    "\n",
    "# def new_delay (self):\n",
    "#     pass\n",
    "\n",
    "# reddit._core._rate_limiter.delay = MethodType(new_delay, reddit._core._rate_limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PRAWJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"Class to encode PRAW objects to JSON.\"\"\"\n",
    "    \"\"\"From https://gist.github.com/jarhill0/6e6495706252d52573950c3820f533b0\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, praw.models.base.PRAWBase):\n",
    "            obj_dict = {}\n",
    "            for key, value in obj.__dict__.items():\n",
    "                if not key.startswith('_'):\n",
    "                    obj_dict[key] = value\n",
    "            return obj_dict\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def digest_reddit_submission (submission):\n",
    "    s = json.loads(json.dumps(submission, cls=PRAWJSONEncoder))\n",
    "    if not reddit_raw.find_one({'id': submission.id}, {'_id': 1}):\n",
    "        return reddit_raw.insert_one(s)\n",
    "    else:\n",
    "        return reddit_raw.update_one({'id': submission.id}, {'$set': s})\n",
    "\n",
    "def reddit_submissions_iter (r, sort, limit):\n",
    "    if sort == 'top_all':\n",
    "        return reddit.subreddit(r).top('all', limit=limit)\n",
    "    elif sort == 'top_year':\n",
    "        return reddit.subreddit(r).top('year', limit=limit)\n",
    "    else:\n",
    "        return reddit.subreddit(r).new(limit=limit)\n",
    "\n",
    "def retrieve_reddit_submissions (r='dataisugly', sort='top_all', limit=100):\n",
    "    return (digest_reddit_submission(s) for s in tqdm(reddit_submissions_iter(r, sort, limit), total=limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# crawler_queue = [\n",
    "#     {'r': 'dataisugly', 'sort': 'top_all', 'limit': 10}\n",
    "# ]\n",
    "\n",
    "reddit_crawler_queue = [\n",
    "    {'r': 'dataisugly', 'sort': 'top_all', 'limit': 1000},\n",
    "    {'r': 'dataisugly', 'sort': 'top_year', 'limit': 1000},\n",
    "    {'r': 'dataisugly', 'sort': 'new', 'limit': 1000}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c6d2ecd25a4d7698e69e47c129e6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50db26567354992b1a6accbcaae1562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37e59aef7a7404998d221c831087b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in reddit_crawler_queue:\n",
    "    [s for s in retrieve_reddit_submissions(r=c['r'], sort=c['sort'], limit=c['limit'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def digest_pushshift_submission (submission):\n",
    "    if not pushshift_raw.find_one({'id': submission['id']}, {'_id': 1}):\n",
    "        return pushshift_raw.insert_one(submission)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def retrieve_pushshift (r, before_epoch, limit=1000):\n",
    "    payload = {\n",
    "        'subreddit': r,\n",
    "        'sort': 'desc',\n",
    "        'sort_type': 'created_utc',\n",
    "        'before': before_epoch,\n",
    "        'size': limit\n",
    "    }\n",
    "    reply = requests.get('https://api.pushshift.io/reddit/search/submission/', params=payload)\n",
    "    if reply.status_code != 200:\n",
    "        print(reply.status_code)\n",
    "        print(json.dumps(payload))\n",
    "    return reply.json()['data']\n",
    "\n",
    "# def retrieve (r, before_epoch, limit=200):\n",
    "#     api = PushshiftAPI(reddit)\n",
    "#     submissions = api.search_submissions(before=before_epoch, subreddit=r, limit=limit)\n",
    "#     return [s for s in submissions]\n",
    "\n",
    "def pushshift_submissions_iter (r, limit=200):\n",
    "    before_epoch = int(datetime(2020, 1, 1).timestamp())\n",
    "    last_submission_id = 0\n",
    "    while True:\n",
    "        submissions = retrieve_pushshift(r, before_epoch, limit)\n",
    "        if last_submission_id == submissions[-1]['id']:\n",
    "            print(f\"Last: {submissions[-1]['id']} {submissions[-1]['created_utc']} {datetime.fromtimestamp(submissions[-1]['created_utc']).isoformat()}\")\n",
    "            break\n",
    "\n",
    "        for submission in submissions:\n",
    "            yield submission\n",
    "\n",
    "        before_epoch = submissions[-1]['created_utc'] + 1\n",
    "        last_submission_id = submissions[-1]['id']\n",
    "        print(f\"New epoch: {before_epoch} {datetime.fromtimestamp(before_epoch).isoformat()}\")\n",
    "\n",
    "def retrieve_pushshift_submissions (r, limit=200):\n",
    "    return (digest_pushshift_submission(s) for s in tqdm(pushshift_submissions_iter(r, limit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pushshift_crawler_queue = [\n",
    "    {'r': 'dataisugly', 'limit': 1000}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610305ef26184993bd244f1d1cb308ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch: 1551741197 2019-03-04T23:13:17\n",
      "New epoch: 1519462761 2018-02-24T08:59:21\n",
      "New epoch: 1490703092 2017-03-28T12:11:32\n",
      "New epoch: 1459722151 2016-04-03T22:22:31\n",
      "New epoch: 1428266609 2015-04-05T20:43:29\n",
      "New epoch: 1353713658 2012-11-23T23:34:18\n",
      "Last: 13otwh 1353713657 2012-11-23T23:34:17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in pushshift_crawler_queue:\n",
    "    submissions = [s for s in retrieve_pushshift_submissions(r=c['r'], limit=c['limit'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to reddit-merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "reddit_merge.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_submission (submission_id):\n",
    "    s = reddit_cache.find_one({'id': submission_id})\n",
    "    if s:\n",
    "        return s\n",
    "\n",
    "    try:\n",
    "        s = reddit.submission(submission_id)\n",
    "        name = s.name # trigger data fetching\n",
    "        s = json.loads(json.dumps(s, cls=PRAWJSONEncoder))\n",
    "    except Exception as e:\n",
    "        print(submission_id, e)\n",
    "        return None\n",
    "\n",
    "    reddit_cache.update_one({'id': submission_id}, {'$setOnInsert': s}, upsert=True)\n",
    "    return s\n",
    "\n",
    "def add_merge (s, duplicated_with=None, retrieve=False):\n",
    "    if not reddit_merge.find_one({'id': s['id']}, {'_id': 1}):\n",
    "        if duplicated_with == None:\n",
    "            duplicated_with = set()\n",
    "\n",
    "        if 'crosspost_parent_list' in s:\n",
    "            duplicated_with = duplicated_with.union([p['id'] for p in s['crosspost_parent_list']])\n",
    "\n",
    "        if retrieve:\n",
    "            submission = retrieve_submission(s['id'])\n",
    "            if not submission:\n",
    "                return 0\n",
    "            if 'permalink' in s:\n",
    "                submission['alt'] = s\n",
    "            s = submission\n",
    "\n",
    "        if 'crosspost_parent_list' in s:\n",
    "            duplicated_with = duplicated_with.union([p['id'] for p in s['crosspost_parent_list']])\n",
    "            del s['crosspost_parent_list']\n",
    "        if 'alt' in s and 'crosspost_parent_list' in s['alt']:\n",
    "            duplicated_with = duplicated_with.union([p['id'] for p in s['alt']['crosspost_parent_list']])\n",
    "            del s['alt']['crosspost_parent_list']\n",
    "\n",
    "        if 'reddit.com/r' in s['url']:\n",
    "            tokens = s['url'].split('/')\n",
    "            if len(tokens) > 6:\n",
    "                duplicated_with.add(tokens[6])\n",
    "\n",
    "        s['duplicated_with'] = list(duplicated_with)\n",
    "        reddit_merge.update_one({'id': s['id']}, {'$setOnInsert': s}, upsert=True)\n",
    "\n",
    "        added = [add_merge({'id': d}, duplicated_with=duplicated_with.union(set([s['id']])), retrieve=True) for d in duplicated_with]\n",
    "\n",
    "        return sum(added) + 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75afaf0c442a4188aef7b1048af42507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5962.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpca3e received 403 HTTP response\n",
      "af71fc received 403 HTTP response\n",
      "\n",
      "Retrieved 6477/5962\n"
     ]
    }
   ],
   "source": [
    "result = parallel(add_merge, pushshift_raw.find(), params_dict={'retrieve': True}, total=pushshift_raw.estimated_document_count(), n_jobs=-1)\n",
    "print(f\"Retrieved {sum(result)}/{len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f01a36a90a64e82afa59d9ba81ae409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2097.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved 690/2097\n"
     ]
    }
   ],
   "source": [
    "result = parallel(add_merge, reddit_raw.find(), total=reddit_raw.estimated_document_count(), n_jobs=-1)\n",
    "print(f\"Retrieved {sum(result)}/{len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ehsr2o', 1), ('eh9a95', 1), ('eh7wox', 1), ('ehafnl', 1), ('ehsqv3', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([s['id'] for s in reddit_merge.find()]).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digest into posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "posts.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import MutableMapping, Sequence\n",
    "from collections import Counter\n",
    "\n",
    "# from https://stackoverflow.com/questions/51488240/python-get-json-keys-as-full-path\n",
    "def get_paths(source):\n",
    "    paths = []\n",
    "    if isinstance(source, MutableMapping):  # found a dict-like structure...\n",
    "        for k, v in source.items():  # iterate over it; Python 2.x: source.iteritems()\n",
    "            paths.append([k])  # add the current child path\n",
    "            paths += [[k] + x for x in get_paths(v)]  # get sub-paths, extend with the current\n",
    "    # else, check if a list-like structure, remove if you don't want list paths included\n",
    "    elif isinstance(source, Sequence) and not isinstance(source, str):\n",
    "        #                          Python 2.x: use basestring instead of str ^\n",
    "        for i, v in enumerate(source):\n",
    "            paths.append([i])\n",
    "            paths += [[i] + x for x in get_paths(v)]  # get sub-paths, extend with the current\n",
    "    return paths\n",
    "\n",
    "c = Counter([str(p) for s in reddit_merge.find() for p in get_paths(s)])\n",
    "with open('reddit_attrs.txt', 'w') as f:\n",
    "    f.write(str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Submission ():\n",
    "    _attrs = [\n",
    "        'id',\n",
    "        'post_id',\n",
    "        'datetime',\n",
    "        'url',\n",
    "        'title',\n",
    "        'content',\n",
    "        'author',\n",
    "        'thumbnail',\n",
    "        'preview',\n",
    "        'removed',\n",
    "        'ups',\n",
    "        'num_comments',\n",
    "        'external_link',\n",
    "        'source',\n",
    "        'source_platform',\n",
    "        'source_url',\n",
    "        'duplicated_posts',\n",
    "        'tags',\n",
    "        'media_type',\n",
    "        'labels',\n",
    "        'alt',\n",
    "        'preview_alt',\n",
    "        'thumbnail_alt',\n",
    "        'external_link_alt'\n",
    "    ]\n",
    "\n",
    "    _video_type = {\n",
    "        'gfycat.com',\n",
    "        'streamable.com',\n",
    "        'vimeo.com',\n",
    "        'youtube.com'\n",
    "    }\n",
    "\n",
    "    def __init__ (self, s, duplicated_post=None):\n",
    "        self._s = s\n",
    "#         self._duplicated_post = duplicated_post\n",
    "        self._subreddit = s['subreddit'] if type(s['subreddit']) == str else s['subreddit']['display_name']\n",
    "\n",
    "        self.id = s['id']\n",
    "        self.post_id = f'reddit/{self._subreddit}/{self.id}'\n",
    "        self.title = s['title']\n",
    "        self.content = s['selftext'] if 'selftext' in s else ''\n",
    "        self.ups = s['ups'] if 'ups' in s else 0\n",
    "        self.num_comments = s['num_comments']\n",
    "        self.source = self._subreddit\n",
    "        self.source_platform = 'reddit'\n",
    "        self.source_url = f'https://www.reddit.com/{s[\"subreddit_name_prefixed\"]}' if 'subreddit_name_prefixed' in s else ''\n",
    "\n",
    "    def digest (self):\n",
    "        return {a:getattr(self, a) for a in Submission._attrs}\n",
    "\n",
    "    @property\n",
    "    def alt (self):\n",
    "        if 'alt' not in self._s:\n",
    "            return None\n",
    "        return Submission(self._s['alt']).digest()\n",
    "\n",
    "    @property\n",
    "    def preview_alt (self):\n",
    "        if self.alt and self.alt['preview']['url'] and self.alt['preview']['url'] != self.preview['url']:\n",
    "            return self.alt['preview']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def thumbnail_alt (self):\n",
    "        if self.alt and self.alt['thumbnail']['url'] and self.alt['thumbnail']['url'] != self.thumbnail['url']:\n",
    "            return self.alt['thumbnail']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def external_link_alt (self):\n",
    "        if self.alt and self.alt['external_link'] and self.alt['external_link'] != self.external_link:\n",
    "            return self.alt['external_link']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def datetime (self):\n",
    "        return datetime.fromtimestamp(self._s['created_utc']).isoformat()\n",
    "\n",
    "    @property\n",
    "    def url (self):\n",
    "        if 'permalink' in self._s:\n",
    "            if self._s['permalink'].startswith('/'):\n",
    "                return f'https://reddit.com{self._s[\"permalink\"]}'\n",
    "            else:\n",
    "                print(f'url: {self._s[\"permalink\"]}')\n",
    "                return self._s['permalink']\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @property\n",
    "    def author (self):\n",
    "        if 'author' in self._s and self._s['author']:\n",
    "            return self._s['author'] if type(self._s['author']) == str else self._s['author']['name']\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @property\n",
    "    def external_link (self):\n",
    "        if 'url' in self._s and self._s['url'] != self.url:\n",
    "            return self._s['url']\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @property\n",
    "    def duplicated_posts (self):\n",
    "        duplicated_posts = []\n",
    "        if 'duplicated_with' not in self._s:\n",
    "            return duplicated_posts\n",
    "\n",
    "        for submission_id in self._s['duplicated_with']:\n",
    "            s = reddit_merge.find_one({'id': submission_id})\n",
    "            if s:\n",
    "                duplicated_posts.append(Submission(s).post_id)\n",
    "        return duplicated_posts\n",
    "#         duplicated_posts = []\n",
    "#         if self._duplicated_post:\n",
    "#             duplicated_posts.append(self._duplicated_post)\n",
    "#         if self._crosspost:\n",
    "#             duplicated_posts.append(self._crosspost.post_id)\n",
    "#         if 'viz.wtf' in self.external_link:\n",
    "#             wtfviz_id = [t for t in self.external_link.split('/') if t.isdigit()]\n",
    "#             duplicated_posts.append(f'tumblr/wtf-viz/{wtfviz_id}')\n",
    "#         return duplicated_posts\n",
    "\n",
    "    @property\n",
    "    def tags (self):\n",
    "        return [self._s['link_flair_text']] if 'link_flair_text' in self._s and self._s['link_flair_text'] else []\n",
    "\n",
    "    @property\n",
    "    def labels (self):\n",
    "        return {\n",
    "            'auto': labels_from_tags(self.tags)\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def media_type (self):\n",
    "        if 'is_video' in self._s and self._s['is_video']:\n",
    "            return 'video'\n",
    "        if 'media' in self._s and self._s['media'] and 'type' in self._s['media']:\n",
    "            if self._s['media']['type'] in Submission._video_type:\n",
    "                return 'video'\n",
    "        if self.preview['url']:\n",
    "            return 'image'\n",
    "        return 'text'\n",
    "\n",
    "    @property\n",
    "    def preview (self):\n",
    "        try:\n",
    "            if 'preview' in self._s:\n",
    "                return {\n",
    "                    'url': self._s['preview']['images'][0]['source']['url'],\n",
    "                    'width': self._s['preview']['images'][0]['source']['width'],\n",
    "                    'height': self._s['preview']['images'][0]['source']['height']\n",
    "                }\n",
    "            elif 'url' in self._s:\n",
    "#                     print(f'Reddit submission preview digest no preview but url: {self._s[\"id\"]} {self.url}')\n",
    "                return {\n",
    "                    'url': '',\n",
    "                    'width': 0,\n",
    "                    'height': 0\n",
    "                }\n",
    "            else:\n",
    "#                     print(f'Reddit submission preview digest no preview nor url: {self._s[\"id\"]}')\n",
    "                return {\n",
    "                    'url': '',\n",
    "                    'width': 0,\n",
    "                    'height': 0\n",
    "                }\n",
    "        except Exception as inst:\n",
    "            print(f'Reddit submission preview digest error: {inst}')\n",
    "            print(f'Reddit submission: {self._s[\"id\"]} {self.url}')\n",
    "            return {\n",
    "                'url': '',\n",
    "                'width': 0,\n",
    "                'height': 0\n",
    "            }\n",
    "\n",
    "    @property\n",
    "    def thumbnail (self):\n",
    "        try:\n",
    "            if 'thumbnail' in self._s:\n",
    "                if not self._s['thumbnail'].startswith('http'):\n",
    "                    if not (self._s['thumbnail'] == 'default' or\n",
    "                            self._s['thumbnail'] == 'self' or\n",
    "                            self._s['thumbnail'] == 'spoiler' or\n",
    "                            self._s['thumbnail'] == 'nsfw' or\n",
    "                            self._s['thumbnail'] == 'image'):\n",
    "                        print(f\"Reddit submission thumbnail invalid url: {self.id} {self._s['thumbnail']}\")\n",
    "                    return {\n",
    "                        'url': '',\n",
    "                        'width': 0,\n",
    "                        'height': 0\n",
    "                    }\n",
    "                else:\n",
    "                    return {\n",
    "                        'url': self._s['thumbnail'],\n",
    "                        'width': self._s.get('thumbnail_width', -1),\n",
    "                        'height': self._s.get('thumbnail_height', -1)\n",
    "                    }\n",
    "            elif 'preview' in self._s:\n",
    "                return {\n",
    "                    'url': self._s['preview']['images'][0]['resolutions'][0]['url'],\n",
    "                    'width': self._s['preview']['images'][0]['resolutions'][0]['width'],\n",
    "                    'height': self._s['preview']['images'][0]['resolutions'][0]['height']\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'url': '',\n",
    "                    'width': 0,\n",
    "                    'height': 0\n",
    "                }\n",
    "        except Exception as inst:\n",
    "            print(f'Reddit submission thumbnail digest error: {inst}')\n",
    "            print(f'Reddit submission: {self._s[\"id\"]} {self.url}')\n",
    "            return {\n",
    "                'url': '',\n",
    "                'width': 0,\n",
    "                'height': 0\n",
    "            }\n",
    "\n",
    "    @property\n",
    "    def removed (self):\n",
    "        return ('removed_by_category' in self._s and not not self._s['removed_by_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def digest_all_submissions ():\n",
    "    for s in tqdm(reddit_merge.find()):\n",
    "        submission = Submission(s)\n",
    "        posts.replace_one({'post_id': submission.post_id}, submission.digest(), upsert=True)\n",
    "#         if submission._crosspost:\n",
    "#             posts.replace_one({'post_id': submission._crosspost.post_id}, submission._crosspost.digest(), upsert=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8359b2a9729f42cc866e23b4a62899bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "digest_all_submissions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix problematic posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ef6128b10a328604705d300'),\n",
       " 'id': '4f927p',\n",
       " 'post_id': 'reddit/dataisugly/4f927p',\n",
       " 'datetime': '2016-04-17T23:25:13',\n",
       " 'url': 'https://reddit.com/r/dataisugly/comments/4f927p/the_truth_about_abuse/',\n",
       " 'title': 'The Truth about Abuse',\n",
       " 'content': '[deleted]',\n",
       " 'author': '',\n",
       " 'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       " 'preview': {'url': '', 'width': 0, 'height': 0},\n",
       " 'removed': False,\n",
       " 'ups': 0,\n",
       " 'num_comments': 0,\n",
       " 'external_link': 'http://imgur.com/(null)',\n",
       " 'source': 'dataisugly',\n",
       " 'source_platform': 'reddit',\n",
       " 'source_url': 'https://www.reddit.com/r/dataisugly',\n",
       " 'duplicated_posts': [],\n",
       " 'tags': [],\n",
       " 'media_type': 'text',\n",
       " 'labels': {'auto': []},\n",
       " 'alt': {'id': '4f927p',\n",
       "  'post_id': 'reddit/dataisugly/4f927p',\n",
       "  'datetime': '2016-04-17T23:25:13',\n",
       "  'url': 'https://reddit.com/r/dataisugly/comments/4f927p/the_truth_about_abuse/',\n",
       "  'title': 'The Truth about Abuse',\n",
       "  'content': '',\n",
       "  'author': 'sharkpony',\n",
       "  'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       "  'preview': {'url': '', 'width': 0, 'height': 0},\n",
       "  'removed': False,\n",
       "  'ups': 0,\n",
       "  'num_comments': 0,\n",
       "  'external_link': 'http://imgur.com/(null)',\n",
       "  'source': 'dataisugly',\n",
       "  'source_platform': 'reddit',\n",
       "  'source_url': '',\n",
       "  'duplicated_posts': [],\n",
       "  'tags': [],\n",
       "  'media_type': 'text',\n",
       "  'labels': {'auto': []},\n",
       "  'alt': None,\n",
       "  'preview_alt': None,\n",
       "  'thumbnail_alt': None,\n",
       "  'external_link_alt': None},\n",
       " 'preview_alt': None,\n",
       " 'thumbnail_alt': None,\n",
       " 'external_link_alt': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.find_one({'id': '4f927p'}) # invalid external_link: imgur.com/(null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f89946b65f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.update_one({'id': '4f927p'},\n",
    "                 {'$set': {\n",
    "                     'skip': True,\n",
    "                     'crawler_remarks': 'invalid link: imgur.com/(null)'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ef6128d10a328604705dc14'),\n",
       " 'id': '2zczew',\n",
       " 'post_id': 'reddit/dataisugly/2zczew',\n",
       " 'datetime': '2015-03-17T15:44:00',\n",
       " 'url': 'https://reddit.com/r/dataisugly/comments/2zczew/the_cost_of_remediation_kind_of/',\n",
       " 'title': 'The cost of remediation... kind of.',\n",
       " 'content': '[deleted]',\n",
       " 'author': '',\n",
       " 'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       " 'preview': {'url': '', 'width': 0, 'height': 0},\n",
       " 'removed': False,\n",
       " 'ups': 1,\n",
       " 'num_comments': 0,\n",
       " 'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)',\n",
       " 'source': 'dataisugly',\n",
       " 'source_platform': 'reddit',\n",
       " 'source_url': 'https://www.reddit.com/r/dataisugly',\n",
       " 'duplicated_posts': [],\n",
       " 'tags': [],\n",
       " 'media_type': 'text',\n",
       " 'labels': {'auto': []},\n",
       " 'alt': {'id': '2zczew',\n",
       "  'post_id': 'reddit/dataisugly/2zczew',\n",
       "  'datetime': '2015-03-17T15:44:00',\n",
       "  'url': 'https://reddit.com/r/dataisugly/comments/2zczew/the_cost_of_remediation_kind_of/',\n",
       "  'title': 'The cost of remediation... kind of.',\n",
       "  'content': '',\n",
       "  'author': '[deleted]',\n",
       "  'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       "  'preview': {'url': '', 'width': 0, 'height': 0},\n",
       "  'removed': False,\n",
       "  'ups': 0,\n",
       "  'num_comments': 0,\n",
       "  'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)',\n",
       "  'source': 'dataisugly',\n",
       "  'source_platform': 'reddit',\n",
       "  'source_url': '',\n",
       "  'duplicated_posts': [],\n",
       "  'tags': [],\n",
       "  'media_type': 'text',\n",
       "  'labels': {'auto': []},\n",
       "  'alt': None,\n",
       "  'preview_alt': None,\n",
       "  'thumbnail_alt': None,\n",
       "  'external_link_alt': None},\n",
       " 'preview_alt': None,\n",
       " 'thumbnail_alt': None,\n",
       " 'external_link_alt': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.find_one({'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)'}) # invalid external_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7f89946b6640>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.update_one({'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)'},\n",
    "                 {'$set': {\n",
    "                     'external_link': 'http://i.imgur.com/ozwpju8.png',\n",
    "                     'crawler_remarks': 'corrected invalid link'}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
