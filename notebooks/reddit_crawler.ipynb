{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install praw\n",
    "# !pip install requests-cache\n",
    "# !pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests_cache\n",
    "!mkdir -p web_cache\n",
    "requests_cache.install_cache('web_cache/redditcrawler')\n",
    "# requests_cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import JSON\n",
    "from collections import Counter\n",
    "\n",
    "from lib.parallel import parallel\n",
    "from lib.labels_from_tags import labels_from_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = json.load(open('./credentials/mongodb_credentials.json'))['uri']\n",
    "mongo = MongoClient(mongo_uri)\n",
    "db = mongo['bad-vis']\n",
    "reddit_raw = db['reddit-raw']\n",
    "pushshift_raw = db['pushshift-raw']\n",
    "reddit_merge = db['reddit-merge']\n",
    "reddit_cache = db['reddit-cache']\n",
    "posts = db['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = \"\"\n",
    "\n",
    "with open('./credentials/reddit_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials['client_id'],\n",
    "                     client_secret=credentials['client_secret'],\n",
    "                     password=credentials['password'],\n",
    "                     user_agent=credentials['user_agent'],\n",
    "                     username=credentials['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the rate limiter since we have requests cache\n",
    "# from types import MethodType\n",
    "\n",
    "# def new_delay (self):\n",
    "#     pass\n",
    "\n",
    "# reddit._core._rate_limiter.delay = MethodType(new_delay, reddit._core._rate_limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRAWJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"Class to encode PRAW objects to JSON.\"\"\"\n",
    "    \"\"\"From https://gist.github.com/jarhill0/6e6495706252d52573950c3820f533b0\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, praw.models.base.PRAWBase):\n",
    "            obj_dict = {}\n",
    "            for key, value in obj.__dict__.items():\n",
    "                if not key.startswith('_'):\n",
    "                    obj_dict[key] = value\n",
    "            return obj_dict\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def digest_reddit_submission (submission):\n",
    "    s = json.loads(json.dumps(submission, cls=PRAWJSONEncoder))\n",
    "    if not reddit_raw.find_one({'id': submission.id}, {'_id': 1}):\n",
    "        reddit_raw.insert_one(s)\n",
    "    else:\n",
    "        reddit_raw.update_one({'id': submission.id}, {'$set': s})\n",
    "    return s\n",
    "\n",
    "def reddit_submissions_iter (r, sort, limit):\n",
    "    if sort == 'top_all':\n",
    "        return reddit.subreddit(r).top('all', limit=limit)\n",
    "    elif sort == 'top_year':\n",
    "        return reddit.subreddit(r).top('year', limit=limit)\n",
    "    else:\n",
    "        return reddit.subreddit(r).new(limit=limit)\n",
    "\n",
    "def retrieve_reddit_submissions (r='dataisugly', sort='top_all', limit=100):\n",
    "    return (digest_reddit_submission(s) for s in tqdm(reddit_submissions_iter(r, sort, limit), total=limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler_queue = [\n",
    "#     {'r': 'dataisugly', 'sort': 'top_all', 'limit': 10}\n",
    "# ]\n",
    "\n",
    "reddit_crawler_queue = [\n",
    "    {'r': 'dataisugly', 'sort': 'top_all', 'limit': 1000},\n",
    "    {'r': 'dataisugly', 'sort': 'top_year', 'limit': 1000},\n",
    "    {'r': 'dataisugly', 'sort': 'new', 'limit': 1000}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780184a40e424fb5b5439b6d693a6e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed4ff50e375496584511ea479c0d9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f887ef4a6524a8d87834a3312ef3489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for c in reddit_crawler_queue:\n",
    "    c['submissions'] = [s for s in retrieve_reddit_submissions(r=c['r'], sort=c['sort'], limit=c['limit'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-07T22:59:25'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromtimestamp(reddit_crawler_queue[2]['submissions'][0]['created']).isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digest_pushshift_submission (submission):\n",
    "    if not pushshift_raw.find_one({'id': submission['id']}, {'_id': 1}):\n",
    "        return pushshift_raw.insert_one(submission)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def retrieve_pushshift (r, before_epoch, limit=1000):\n",
    "    payload = {\n",
    "        'subreddit': r,\n",
    "        'sort': 'desc',\n",
    "        'sort_type': 'created_utc',\n",
    "        'before': before_epoch,\n",
    "        'size': limit\n",
    "    }\n",
    "    reply = requests.get('https://api.pushshift.io/reddit/search/submission/', params=payload)\n",
    "    if reply.status_code != 200:\n",
    "        print(reply.status_code)\n",
    "        print(json.dumps(payload))\n",
    "    return reply.json()['data']\n",
    "\n",
    "# def retrieve (r, before_epoch, limit=200):\n",
    "#     api = PushshiftAPI(reddit)\n",
    "#     submissions = api.search_submissions(before=before_epoch, subreddit=r, limit=limit)\n",
    "#     return [s for s in submissions]\n",
    "\n",
    "def pushshift_submissions_iter (r, limit=200):\n",
    "    before_epoch = int(datetime(2021, 1, 1).timestamp())\n",
    "    # before_epoch = int(datetime.now().timestamp()) # using now() will make the query payload keep shifting, bad for caching\n",
    "    last_submission_id = 0\n",
    "    while True:\n",
    "        submissions = retrieve_pushshift(r, before_epoch, limit)\n",
    "        if last_submission_id == submissions[-1]['id']:\n",
    "            print(f\"Last: {submissions[-1]['id']} {submissions[-1]['created_utc']} {datetime.fromtimestamp(submissions[-1]['created_utc']).isoformat()}\")\n",
    "            break\n",
    "\n",
    "        for submission in submissions:\n",
    "            yield submission\n",
    "\n",
    "        before_epoch = submissions[-1]['created_utc'] + 1\n",
    "        last_submission_id = submissions[-1]['id']\n",
    "        print(f\"New epoch: {before_epoch} {datetime.fromtimestamp(before_epoch).isoformat()}\")\n",
    "\n",
    "def retrieve_pushshift_submissions (r, limit=200):\n",
    "    return (digest_pushshift_submission(s) for s in tqdm(pushshift_submissions_iter(r, limit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushshift_crawler_queue = [\n",
    "    {'r': 'dataisugly', 'limit': 1000}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f03c4c30ab94d1ea78c9290d5f064bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch: 1607098254 2020-12-04T16:10:54\n",
      "New epoch: 1605098818 2020-11-11T12:46:58\n",
      "New epoch: 1603206827 2020-10-20T15:13:47\n",
      "New epoch: 1600739867 2020-09-22T01:57:47\n",
      "New epoch: 1598532092 2020-08-27T12:41:32\n",
      "New epoch: 1597342464 2020-08-13T18:14:24\n",
      "New epoch: 1595523548 2020-07-23T16:59:08\n",
      "New epoch: 1594004109 2020-07-06T02:55:09\n",
      "New epoch: 1592565887 2020-06-19T11:24:47\n",
      "New epoch: 1591403377 2020-06-06T00:29:37\n",
      "New epoch: 1590017008 2020-05-20T23:23:28\n",
      "New epoch: 1589025604 2020-05-09T12:00:04\n",
      "New epoch: 1587664550 2020-04-23T17:55:50\n",
      "New epoch: 1586451386 2020-04-09T16:56:26\n",
      "New epoch: 1585659891 2020-03-31T13:04:51\n",
      "New epoch: 1584536387 2020-03-18T12:59:47\n",
      "New epoch: 1583287203 2020-03-04T02:00:03\n",
      "New epoch: 1581922301 2020-02-17T06:51:41\n",
      "New epoch: 1580127925 2020-01-27T12:25:25\n",
      "New epoch: 1578134834 2020-01-04T10:47:14\n",
      "New epoch: 1575557588 2019-12-05T14:53:08\n",
      "New epoch: 1573142196 2019-11-07T15:56:36\n",
      "New epoch: 1570254869 2019-10-05T05:54:29\n",
      "New epoch: 1566666666 2019-08-24T17:11:06\n",
      "New epoch: 1563786399 2019-07-22T09:06:39\n",
      "New epoch: 1560700920 2019-06-16T16:02:00\n",
      "New epoch: 1558381331 2019-05-20T19:42:11\n",
      "New epoch: 1556488263 2019-04-28T21:51:03\n",
      "New epoch: 1554584217 2019-04-06T20:56:57\n",
      "New epoch: 1552383560 2019-03-12T09:39:20\n",
      "New epoch: 1549651799 2019-02-08T18:49:59\n",
      "New epoch: 1546212069 2018-12-30T23:21:09\n",
      "New epoch: 1542004852 2018-11-12T06:40:52\n",
      "New epoch: 1538188411 2018-09-29T02:33:31\n",
      "New epoch: 1534547409 2018-08-17T23:10:09\n",
      "New epoch: 1532227088 2018-07-22T02:38:08\n",
      "New epoch: 1528817741 2018-06-12T15:35:41\n",
      "New epoch: 1526329387 2018-05-14T20:23:07\n",
      "New epoch: 1523041392 2018-04-06T19:03:12\n",
      "New epoch: 1520440630 2018-03-07T16:37:10\n",
      "New epoch: 1517403572 2018-01-31T12:59:32\n",
      "New epoch: 1513962385 2017-12-22T17:06:25\n",
      "New epoch: 1510772496 2017-11-15T19:01:36\n",
      "New epoch: 1507733283 2017-10-11T14:48:03\n",
      "New epoch: 1505499647 2017-09-15T18:20:47\n",
      "New epoch: 1502965323 2017-08-17T10:22:03\n",
      "New epoch: 1499389648 2017-07-07T01:07:28\n",
      "New epoch: 1496418321 2017-06-02T15:45:21\n",
      "New epoch: 1493602212 2017-05-01T01:30:12\n",
      "New epoch: 1491683274 2017-04-08T20:27:54\n",
      "New epoch: 1489575836 2017-03-15T11:03:56\n",
      "New epoch: 1486991578 2017-02-13T13:12:58\n",
      "New epoch: 1484334986 2017-01-13T19:16:26\n",
      "New epoch: 1481226629 2016-12-08T19:50:29\n",
      "New epoch: 1478507145 2016-11-07T08:25:45\n",
      "New epoch: 1475575091 2016-10-04T09:58:11\n",
      "New epoch: 1473106498 2016-09-05T20:14:58\n",
      "New epoch: 1470086962 2016-08-01T21:29:22\n",
      "New epoch: 1465760678 2016-06-12T19:44:38\n",
      "New epoch: 1461174998 2016-04-20T17:56:38\n",
      "New epoch: 1458146867 2016-03-16T16:47:47\n",
      "New epoch: 1456018590 2016-02-21T01:36:30\n",
      "New epoch: 1453766137 2016-01-25T23:55:37\n",
      "New epoch: 1449696657 2015-12-09T21:30:57\n",
      "New epoch: 1445890616 2015-10-26T20:16:56\n",
      "New epoch: 1442259244 2015-09-14T19:34:04\n",
      "New epoch: 1439116716 2015-08-09T10:38:36\n",
      "New epoch: 1435543494 2015-06-29T02:04:54\n",
      "New epoch: 1432194698 2015-05-21T07:51:38\n",
      "New epoch: 1429977040 2015-04-25T15:50:40\n",
      "New epoch: 1427186722 2015-03-24T08:45:22\n",
      "New epoch: 1424798307 2015-02-24T17:18:27\n",
      "New epoch: 1421559948 2015-01-18T05:45:48\n",
      "New epoch: 1417637880 2014-12-03T20:18:00\n",
      "New epoch: 1414098327 2014-10-23T21:05:27\n",
      "New epoch: 1409625166 2014-09-02T02:32:46\n",
      "New epoch: 1404091336 2014-06-30T01:22:16\n",
      "New epoch: 1398714775 2014-04-28T19:52:55\n",
      "New epoch: 1391957976 2014-02-09T14:59:36\n",
      "New epoch: 1372273080 2013-06-26T18:58:00\n",
      "New epoch: 1353713658 2012-11-23T23:34:18\n",
      "Last: 13otwh 1353713657 2012-11-23T23:34:17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in pushshift_crawler_queue:\n",
    "    submissions = [s for s in retrieve_pushshift_submissions(r=c['r'], limit=c['limit'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to reddit-merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id_1'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_merge.drop()\n",
    "reddit_merge.create_index('id', unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_submission (submission_id):\n",
    "    s = reddit_cache.find_one({'id': submission_id})\n",
    "    if s:\n",
    "        return s\n",
    "\n",
    "    try:\n",
    "        s = reddit.submission(submission_id)\n",
    "        name = s.name # trigger data fetching\n",
    "        s = json.loads(json.dumps(s, cls=PRAWJSONEncoder))\n",
    "    except Exception as e:\n",
    "        print(submission_id, e)\n",
    "        return None\n",
    "\n",
    "    reddit_cache.update_one({'id': submission_id}, {'$setOnInsert': s}, upsert=True)\n",
    "    return s\n",
    "\n",
    "def add_merge (s, duplicated_with=None, retrieve=False):\n",
    "    if not reddit_merge.find_one({'id': s['id']}, {'_id': 1}):\n",
    "        if duplicated_with == None:\n",
    "            duplicated_with = set()\n",
    "\n",
    "        if 'crosspost_parent_list' in s:\n",
    "            duplicated_with = duplicated_with.union([p['id'] for p in s['crosspost_parent_list']])\n",
    "\n",
    "        if retrieve:\n",
    "            submission = retrieve_submission(s['id'])\n",
    "            if not submission:\n",
    "                return 0\n",
    "            if 'permalink' in s:\n",
    "                submission['alt'] = s\n",
    "            s = submission\n",
    "\n",
    "        if 'crosspost_parent_list' in s:\n",
    "            duplicated_with = duplicated_with.union([p['id'] for p in s['crosspost_parent_list']])\n",
    "            del s['crosspost_parent_list']\n",
    "        if 'alt' in s and 'crosspost_parent_list' in s['alt']:\n",
    "            duplicated_with = duplicated_with.union([p['id'] for p in s['alt']['crosspost_parent_list']])\n",
    "            del s['alt']['crosspost_parent_list']\n",
    "\n",
    "        if 'reddit.com/r' in s['url']:\n",
    "            tokens = s['url'].split('/')\n",
    "            if len(tokens) > 6:\n",
    "                duplicated_with.add(tokens[6])\n",
    "\n",
    "        s['duplicated_with'] = list(duplicated_with)\n",
    "        if '_id' in s:\n",
    "            del s['_id']\n",
    "        reddit_merge.update_one({'id': s['id']}, {'$setOnInsert': s}, upsert=True)\n",
    "\n",
    "        added = [add_merge({'id': d}, duplicated_with=duplicated_with.union(set([s['id']])), retrieve=True) for d in duplicated_with]\n",
    "\n",
    "        return sum(added) + 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c59c5bb42614025a34450634173892b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7958.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpca3e received 403 HTTP response\n",
      "af71fc received 403 HTTP response\n",
      "\n",
      "Retrieved 8826/7958\n"
     ]
    }
   ],
   "source": [
    "result = parallel(add_merge, list(pushshift_raw.find()), params_dict={'retrieve': True}, total=pushshift_raw.estimated_document_count(), n_jobs=-1)\n",
    "print(f\"Retrieved {sum(result)}/{len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e438b8b89cfb407fb0b25d93ce686b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1721.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved 33/1721\n"
     ]
    }
   ],
   "source": [
    "result = parallel(add_merge, reddit_raw.find(), total=reddit_raw.estimated_document_count(), n_jobs=-1)\n",
    "print(f\"Retrieved {sum(result)}/{len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ehsr2o', 1), ('ehsqv3', 1), ('ehafnl', 1), ('eh9a95', 1), ('eh7yy8', 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if duplicated posts inserted by racing condition\n",
    "Counter([s['id'] for s in reddit_merge.find()]).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digest into posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7fd9c9a65380>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.delete_many({'source_platform': 'reddit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import MutableMapping, Sequence\n",
    "from collections import Counter\n",
    "\n",
    "# from https://stackoverflow.com/questions/51488240/python-get-json-keys-as-full-path\n",
    "def get_paths(source):\n",
    "    paths = []\n",
    "    if isinstance(source, MutableMapping):  # found a dict-like structure...\n",
    "        for k, v in source.items():  # iterate over it; Python 2.x: source.iteritems()\n",
    "            paths.append([k])  # add the current child path\n",
    "            paths += [[k] + x for x in get_paths(v)]  # get sub-paths, extend with the current\n",
    "    # else, check if a list-like structure, remove if you don't want list paths included\n",
    "    elif isinstance(source, Sequence) and not isinstance(source, str):\n",
    "        #                          Python 2.x: use basestring instead of str ^\n",
    "        for i, v in enumerate(source):\n",
    "            paths.append([i])\n",
    "            paths += [[i] + x for x in get_paths(v)]  # get sub-paths, extend with the current\n",
    "    return paths\n",
    "\n",
    "c = Counter([str(p) for s in reddit_merge.find() for p in get_paths(s)])\n",
    "with open('reddit_attrs.txt', 'w') as f:\n",
    "    f.write(str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submission ():\n",
    "    _attrs = [\n",
    "        'id',\n",
    "        'post_id',\n",
    "        'datetime',\n",
    "        'url',\n",
    "        'title',\n",
    "        'content',\n",
    "        'author',\n",
    "        'thumbnail',\n",
    "        'preview',\n",
    "        'removed',\n",
    "        'ups',\n",
    "        'num_comments',\n",
    "        'external_link',\n",
    "        'source',\n",
    "        'source_platform',\n",
    "        'source_url',\n",
    "        'duplicated_posts',\n",
    "        'tags',\n",
    "        'media_type',\n",
    "        'labels',\n",
    "        'alt',\n",
    "        'preview_alt',\n",
    "        'thumbnail_alt',\n",
    "        'external_link_alt'\n",
    "    ]\n",
    "\n",
    "    _video_type = {\n",
    "        'gfycat.com',\n",
    "        'streamable.com',\n",
    "        'vimeo.com',\n",
    "        'youtube.com'\n",
    "    }\n",
    "\n",
    "    def __init__ (self, s, duplicated_post=None):\n",
    "        self._s = s\n",
    "#         self._duplicated_post = duplicated_post\n",
    "        self._subreddit = s['subreddit'] if type(s['subreddit']) == str else s['subreddit']['display_name']\n",
    "\n",
    "        self.id = s['id']\n",
    "        self.post_id = f'reddit/{self._subreddit}/{self.id}'\n",
    "        self.title = s['title']\n",
    "        self.content = s['selftext'] if 'selftext' in s else ''\n",
    "        self.ups = s['ups'] if 'ups' in s else 0\n",
    "        self.num_comments = s['num_comments']\n",
    "        self.source = self._subreddit\n",
    "        self.source_platform = 'reddit'\n",
    "        self.source_url = f'https://www.reddit.com/{s[\"subreddit_name_prefixed\"]}' if 'subreddit_name_prefixed' in s else ''\n",
    "\n",
    "    def digest (self):\n",
    "        return {a:getattr(self, a) for a in Submission._attrs}\n",
    "\n",
    "    @property\n",
    "    def alt (self):\n",
    "        if 'alt' not in self._s:\n",
    "            return None\n",
    "        return Submission(self._s['alt']).digest()\n",
    "\n",
    "    @property\n",
    "    def preview_alt (self):\n",
    "        if self.alt and self.alt['preview']['url'] and self.alt['preview']['url'] != self.preview['url']:\n",
    "            return self.alt['preview']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def thumbnail_alt (self):\n",
    "        if self.alt and self.alt['thumbnail']['url'] and self.alt['thumbnail']['url'] != self.thumbnail['url']:\n",
    "            return self.alt['thumbnail']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def external_link_alt (self):\n",
    "        if self.alt and self.alt['external_link'] and self.alt['external_link'] != self.external_link:\n",
    "            return self.alt['external_link']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def datetime (self):\n",
    "        return datetime.fromtimestamp(self._s['created_utc']).isoformat()\n",
    "\n",
    "    @property\n",
    "    def url (self):\n",
    "        if 'permalink' in self._s:\n",
    "            if self._s['permalink'].startswith('/'):\n",
    "                return f'https://reddit.com{self._s[\"permalink\"]}'\n",
    "            else:\n",
    "                print(f'url: {self._s[\"permalink\"]}')\n",
    "                return self._s['permalink']\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @property\n",
    "    def author (self):\n",
    "        if 'author' in self._s and self._s['author']:\n",
    "            return self._s['author'] if type(self._s['author']) == str else self._s['author']['name']\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @property\n",
    "    def external_link (self):\n",
    "        if 'url' in self._s and self._s['url'] != self.url:\n",
    "            return self._s['url']\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @property\n",
    "    def duplicated_posts (self):\n",
    "        duplicated_posts = []\n",
    "        if 'duplicated_with' not in self._s:\n",
    "            return duplicated_posts\n",
    "\n",
    "        for submission_id in self._s['duplicated_with']:\n",
    "            s = reddit_merge.find_one({'id': submission_id})\n",
    "            if s:\n",
    "                duplicated_posts.append(Submission(s).post_id)\n",
    "        return duplicated_posts\n",
    "#         duplicated_posts = []\n",
    "#         if self._duplicated_post:\n",
    "#             duplicated_posts.append(self._duplicated_post)\n",
    "#         if self._crosspost:\n",
    "#             duplicated_posts.append(self._crosspost.post_id)\n",
    "#         if 'viz.wtf' in self.external_link:\n",
    "#             wtfviz_id = [t for t in self.external_link.split('/') if t.isdigit()]\n",
    "#             duplicated_posts.append(f'tumblr/wtf-viz/{wtfviz_id}')\n",
    "#         return duplicated_posts\n",
    "\n",
    "    @property\n",
    "    def tags (self):\n",
    "        return [self._s['link_flair_text']] if 'link_flair_text' in self._s and self._s['link_flair_text'] else []\n",
    "\n",
    "    @property\n",
    "    def labels (self):\n",
    "        return {\n",
    "            'auto': labels_from_tags(self.tags)\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def media_type (self):\n",
    "        if 'is_video' in self._s and self._s['is_video']:\n",
    "            return 'video'\n",
    "        if 'media' in self._s and self._s['media'] and 'type' in self._s['media']:\n",
    "            if self._s['media']['type'] in Submission._video_type:\n",
    "                return 'video'\n",
    "        if self.preview['url']:\n",
    "            return 'image'\n",
    "        return 'text'\n",
    "\n",
    "    @property\n",
    "    def preview (self):\n",
    "        try:\n",
    "            if 'preview' in self._s:\n",
    "                return {\n",
    "                    'url': self._s['preview']['images'][0]['source']['url'],\n",
    "                    'width': self._s['preview']['images'][0]['source']['width'],\n",
    "                    'height': self._s['preview']['images'][0]['source']['height']\n",
    "                }\n",
    "            elif 'url' in self._s:\n",
    "#                     print(f'Reddit submission preview digest no preview but url: {self._s[\"id\"]} {self.url}')\n",
    "                return {\n",
    "                    'url': '',\n",
    "                    'width': 0,\n",
    "                    'height': 0\n",
    "                }\n",
    "            else:\n",
    "#                     print(f'Reddit submission preview digest no preview nor url: {self._s[\"id\"]}')\n",
    "                return {\n",
    "                    'url': '',\n",
    "                    'width': 0,\n",
    "                    'height': 0\n",
    "                }\n",
    "        except Exception as inst:\n",
    "            print(f'Reddit submission preview digest error: {inst}')\n",
    "            print(f'Reddit submission: {self._s[\"id\"]} {self.url}')\n",
    "            return {\n",
    "                'url': '',\n",
    "                'width': 0,\n",
    "                'height': 0\n",
    "            }\n",
    "\n",
    "    @property\n",
    "    def thumbnail (self):\n",
    "        try:\n",
    "            if 'thumbnail' in self._s:\n",
    "                if not self._s['thumbnail'].startswith('http'):\n",
    "                    if not (self._s['thumbnail'] == 'default' or\n",
    "                            self._s['thumbnail'] == 'self' or\n",
    "                            self._s['thumbnail'] == 'spoiler' or\n",
    "                            self._s['thumbnail'] == 'nsfw' or\n",
    "                            self._s['thumbnail'] == 'image'):\n",
    "                        print(f\"Reddit submission thumbnail invalid url: {self.id} {self._s['thumbnail']}\")\n",
    "                    return {\n",
    "                        'url': '',\n",
    "                        'width': 0,\n",
    "                        'height': 0\n",
    "                    }\n",
    "                else:\n",
    "                    return {\n",
    "                        'url': self._s['thumbnail'],\n",
    "                        'width': self._s.get('thumbnail_width', -1),\n",
    "                        'height': self._s.get('thumbnail_height', -1)\n",
    "                    }\n",
    "            elif 'preview' in self._s:\n",
    "                return {\n",
    "                    'url': self._s['preview']['images'][0]['resolutions'][0]['url'],\n",
    "                    'width': self._s['preview']['images'][0]['resolutions'][0]['width'],\n",
    "                    'height': self._s['preview']['images'][0]['resolutions'][0]['height']\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'url': '',\n",
    "                    'width': 0,\n",
    "                    'height': 0\n",
    "                }\n",
    "        except Exception as inst:\n",
    "            print(f'Reddit submission thumbnail digest error: {inst}')\n",
    "            print(f'Reddit submission: {self._s[\"id\"]} {self.url}')\n",
    "            return {\n",
    "                'url': '',\n",
    "                'width': 0,\n",
    "                'height': 0\n",
    "            }\n",
    "\n",
    "    @property\n",
    "    def removed (self):\n",
    "        return ('removed_by_category' in self._s and not not self._s['removed_by_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digest_all_submissions ():\n",
    "    for s in tqdm(reddit_merge.find()):\n",
    "        submission = Submission(s)\n",
    "        posts.replace_one({'post_id': submission.post_id}, submission.digest(), upsert=True)\n",
    "#         if submission._crosspost:\n",
    "#             posts.replace_one({'post_id': submission._crosspost.post_id}, submission._crosspost.digest(), upsert=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e9aa4b7edc4d588ea0c6338d93b598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "digest_all_submissions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix problematic posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ff74a36095476d294c68320'),\n",
       " 'id': '4f927p',\n",
       " 'post_id': 'reddit/dataisugly/4f927p',\n",
       " 'datetime': '2016-04-17T23:25:13',\n",
       " 'url': 'https://reddit.com/r/dataisugly/comments/4f927p/the_truth_about_abuse/',\n",
       " 'title': 'The Truth about Abuse',\n",
       " 'content': '[deleted]',\n",
       " 'author': '',\n",
       " 'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       " 'preview': {'url': '', 'width': 0, 'height': 0},\n",
       " 'removed': False,\n",
       " 'ups': 0,\n",
       " 'num_comments': 0,\n",
       " 'external_link': 'http://imgur.com/(null)',\n",
       " 'source': 'dataisugly',\n",
       " 'source_platform': 'reddit',\n",
       " 'source_url': 'https://www.reddit.com/r/dataisugly',\n",
       " 'duplicated_posts': [],\n",
       " 'tags': [],\n",
       " 'media_type': 'text',\n",
       " 'labels': {'auto': []},\n",
       " 'alt': {'id': '4f927p',\n",
       "  'post_id': 'reddit/dataisugly/4f927p',\n",
       "  'datetime': '2016-04-17T23:25:13',\n",
       "  'url': 'https://reddit.com/r/dataisugly/comments/4f927p/the_truth_about_abuse/',\n",
       "  'title': 'The Truth about Abuse',\n",
       "  'content': '',\n",
       "  'author': 'sharkpony',\n",
       "  'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       "  'preview': {'url': '', 'width': 0, 'height': 0},\n",
       "  'removed': False,\n",
       "  'ups': 0,\n",
       "  'num_comments': 0,\n",
       "  'external_link': 'http://imgur.com/(null)',\n",
       "  'source': 'dataisugly',\n",
       "  'source_platform': 'reddit',\n",
       "  'source_url': '',\n",
       "  'duplicated_posts': [],\n",
       "  'tags': [],\n",
       "  'media_type': 'text',\n",
       "  'labels': {'auto': []},\n",
       "  'alt': None,\n",
       "  'preview_alt': None,\n",
       "  'thumbnail_alt': None,\n",
       "  'external_link_alt': None},\n",
       " 'preview_alt': None,\n",
       " 'thumbnail_alt': None,\n",
       " 'external_link_alt': None}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.find_one({'id': '4f927p'}) # invalid external_link: imgur.com/(null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7fd9b2b0b5c0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.update_one({'id': '4f927p'},\n",
    "                 {'$set': {\n",
    "                     'skip': True,\n",
    "                     'crawler_remarks': 'invalid link: imgur.com/(null)'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5ff74a39095476d294c68c34'),\n",
       " 'id': '2zczew',\n",
       " 'post_id': 'reddit/dataisugly/2zczew',\n",
       " 'datetime': '2015-03-17T15:44:00',\n",
       " 'url': 'https://reddit.com/r/dataisugly/comments/2zczew/the_cost_of_remediation_kind_of/',\n",
       " 'title': 'The cost of remediation... kind of.',\n",
       " 'content': '[deleted]',\n",
       " 'author': '',\n",
       " 'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       " 'preview': {'url': '', 'width': 0, 'height': 0},\n",
       " 'removed': False,\n",
       " 'ups': 1,\n",
       " 'num_comments': 0,\n",
       " 'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)',\n",
       " 'source': 'dataisugly',\n",
       " 'source_platform': 'reddit',\n",
       " 'source_url': 'https://www.reddit.com/r/dataisugly',\n",
       " 'duplicated_posts': [],\n",
       " 'tags': [],\n",
       " 'media_type': 'text',\n",
       " 'labels': {'auto': []},\n",
       " 'alt': {'id': '2zczew',\n",
       "  'post_id': 'reddit/dataisugly/2zczew',\n",
       "  'datetime': '2015-03-17T15:44:00',\n",
       "  'url': 'https://reddit.com/r/dataisugly/comments/2zczew/the_cost_of_remediation_kind_of/',\n",
       "  'title': 'The cost of remediation... kind of.',\n",
       "  'content': '',\n",
       "  'author': '[deleted]',\n",
       "  'thumbnail': {'url': '', 'width': 0, 'height': 0},\n",
       "  'preview': {'url': '', 'width': 0, 'height': 0},\n",
       "  'removed': False,\n",
       "  'ups': 0,\n",
       "  'num_comments': 0,\n",
       "  'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)',\n",
       "  'source': 'dataisugly',\n",
       "  'source_platform': 'reddit',\n",
       "  'source_url': '',\n",
       "  'duplicated_posts': [],\n",
       "  'tags': [],\n",
       "  'media_type': 'text',\n",
       "  'labels': {'auto': []},\n",
       "  'alt': None,\n",
       "  'preview_alt': None,\n",
       "  'thumbnail_alt': None,\n",
       "  'external_link_alt': None},\n",
       " 'preview_alt': None,\n",
       " 'thumbnail_alt': None,\n",
       " 'external_link_alt': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.find_one({'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)'}) # invalid external_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x7fd9b25c9c00>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.update_one({'external_link': 'http://[Imgur](http://i.imgur.com/ozwpju8.png)'},\n",
    "                 {'$set': {\n",
    "                     'external_link': 'http://i.imgur.com/ozwpju8.png',\n",
    "                     'crawler_remarks': 'corrected invalid link'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
