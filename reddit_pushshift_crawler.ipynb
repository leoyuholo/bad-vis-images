{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests_cache\n",
    "requests_cache.install_cache('web_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo = MongoClient('172.17.0.1', 27017)\n",
    "db = mongo['bad-vis']\n",
    "raw = db['pushshift-raw']\n",
    "reddit_raw = db['reddit-raw']\n",
    "reddit_merge = db['reddit-merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_queue = [\n",
    "    {'r': 'dataisugly', 'limit': 1000}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digest_submission (submission):\n",
    "    if not raw.find_one({'id': submission['id']}, {'_id': 1}):\n",
    "        return raw.insert_one(submission)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def retrieve_pushshift (r, before_epoch, limit=1000):\n",
    "    payload = {\n",
    "        'subreddit': r,\n",
    "        'sort': 'desc',\n",
    "        'sort_type': 'created_utc',\n",
    "        'before': before_epoch,\n",
    "        'size': limit\n",
    "    }\n",
    "    reply = requests.get('https://api.pushshift.io/reddit/search/submission/', params=payload)\n",
    "    if reply.status_code != 200:\n",
    "        print(reply.status_code)\n",
    "        print(json.dumps(payload))\n",
    "    return reply.json()['data']\n",
    "    \n",
    "def retrieve (r, before_epoch, limit=200):\n",
    "    api = PushshiftAPI(reddit)\n",
    "    submissions = api.search_submissions(before=before_epoch, subreddit=r, limit=limit)\n",
    "    return [submission for submission in submissions]\n",
    "\n",
    "def submissions_iter (r, limit=200):\n",
    "    before_epoch = int(datetime.datetime(2020, 1, 1).timestamp())\n",
    "    last_submission_id = 0\n",
    "    while True:\n",
    "        submissions = retrieve_pushshift(r, before_epoch, limit)\n",
    "        if last_submission_id == submissions[-1]['id']:\n",
    "            print(f'Last: {submissions[-1][\"id\"]} {submissions[-1][\"created_utc\"]}')\n",
    "            break\n",
    "\n",
    "        for submission in submissions:\n",
    "            yield submission\n",
    "\n",
    "        before_epoch = submissions[-1]['created_utc'] + 1\n",
    "        last_submission_id = submissions[-1]['id']\n",
    "        print(f'New epoch: {before_epoch} {datetime.datetime.fromtimestamp(before_epoch).isoformat()}')\n",
    "\n",
    "def retrieve_submissions (r, limit=200):\n",
    "    return (digest_submission(s) for s in tqdm(submissions_iter(r, limit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0771e3f8007b4f669012d788918236df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch: 1551741197 2019-03-04T23:13:17\n",
      "New epoch: 1519160110 2018-02-20T20:55:10\n",
      "New epoch: 1490509975 2017-03-26T06:32:55\n",
      "New epoch: 1459306492 2016-03-30T02:54:52\n",
      "New epoch: 1428012462 2015-04-02T22:07:42\n",
      "New epoch: 1353713658 2012-11-23T23:34:18\n",
      "Last: 13otwh 1353713657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in crawler_queue:\n",
    "    submissions = [s for s in retrieve_submissions(r=c['r'], limit=c['limit'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with reddit-raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = \"\"\n",
    "\n",
    "with open('./reddit_credentials.json') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=credentials['client_id'],\n",
    "                     client_secret=credentials['client_secret'],\n",
    "                     password=credentials['password'],\n",
    "                     user_agent=credentials['user_agent'],\n",
    "                     username=credentials['username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRAWJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"Class to encode PRAW objects to JSON.\"\"\"\n",
    "    \"\"\"From https://gist.github.com/jarhill0/6e6495706252d52573950c3820f533b0\"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, praw.models.base.PRAWBase):\n",
    "            obj_dict = {}\n",
    "            for key, value in obj.__dict__.items():\n",
    "                if not key.startswith('_'):\n",
    "                    obj_dict[key] = value\n",
    "            return obj_dict\n",
    "        elif 'praw' in str(type(obj)):\n",
    "            return\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def retrieve_original_submissions ():\n",
    "    for s in tqdm(raw.find()):\n",
    "        if not reddit_raw.find_one({'id': s['id']}, {'_id': 1}) and not reddit_merge.find_one({'id': s['id']}, {'_id': 1}):\n",
    "            original = reddit.submission(s['id'])\n",
    "            name = original.name # trigger data fetching\n",
    "            reddit_merge.insert_one(json.loads(json.dumps(original, cls=PRAWJSONEncoder)))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120519b6f48740bc96fdc8d090c09b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieve_original_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c753ddeede34b4dac201ef337976c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in tqdm(reddit_raw.find()):\n",
    "    reddit_merge.insert_one(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
